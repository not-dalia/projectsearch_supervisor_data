---
layout: supervisor
title: Paul Vickers
available: true
email: paul.vickers@northumbria.ac.uk
website: https://paulvickers.github.io/
office_hours: TBC, but you can contact me by email or Teams if I am not in my office.
module:
  - KV6003
research_group: Northumbria Social Computing
research_themes:
  - Sonification and Auditory Display
  - Computer Audio
  - Cyber Security
additional_keywords:
  - Human-Data Interaction
technologies_languages:
  - Pure Data
  - SuperCollider
  - Max/MSP
  - Python
additional_details: >-
  #### Data Sonification: Investigative & General Computing Projects


  [Sonification](https://en.wikipedia.org/wiki/Sonification) is the use of non-speech audio to communicate data or data relations to a listener; think of information visualisation but with sound instead of graphics. I am interested in supervising students who would like to explore problems that could be solved by applying sonification techniques. For an idea of the kinds of things I have been working on recently, have a look at [my personal research page](https://paulvickers.github.io/projects/). We can implement sonifications using free general programming tools such as Python and also using free audio programming languages like [Pure Data](http://puredata.info/) and [SuperCollider](https://supercollider.github.io/). These tools are multi platform so will work on macOS, Linux, and Windows.


  I am the Principal Investigator for a three-year project funded by the Leverhulme Trust titled RADICAL ([info here](https://projectRadical.github.io/about/)) , so I would be very keen to involve you in that project. If you are able to design a sonification that we can use in our experiments, then you would be included as an author on any publications we submit that make use of it.


  For a good overview of sonification, download (for free) the [Sonification Handbook](https://sonification.de/handbook/) and also pay a visit to the website of the [International Community for Auditory Display (ICAD)](http://icad.org/). In June 2019 I hosted [ICAD 2019, The 25th International Conference on Auditory Display](https://icad2019.icad.org/) here at Northumbria, the [proceedings](https://smartech.gatech.edu/handle/1853/61548) of which will give you a flavour of the latest research being done in the field. 


  ##### Project ideas


  1. 3D Sonification: The department has recently procured an [IKO 3D audio speaker](http://iko.sonible.com/en.html) which is one of only two in the UK. A number of really exciting projects would use the IKO as a means of projecting a spatialised sonification of a dataset (either real time or historical). For example:

     a. A real-time sonification of sensor data from the [Newcastle Urban Observatory](http://newcastle.urbanobservatory.ac.uk/) to provide a three-dimensional live representation of local air pollution, weather, traffic volumes, etc. Will require use of Web APIs (csv or JSON format).

     b. It *might* be possible to work with data from the [BESSY II photon synchrotron at the Helmholtz Centre, Berlin](https://www.helmholtz-berlin.de/quellen/bessy/elektronenspeicherring/index_en.html) which would be a really exciting collaboration.

     c. Or perhaps you would like to sonify malicious network activity in real time (see our [SoNSTAR project](https://paulvickers.github.io/SoNSTAR/)) but in a 3D space such that the different sonification components move around the space in response to network activity? The IKO uses ambisonic filters, so this would be a good opportunity for someone wishing to learn more about spatialised digital audio.
  2. Sonification of biomechanical data: I have done some research into the sonification of data generated when using a [physical exercise machine](https://paulvickers.github.io/SoniFRED/). This has opened up a range of questions that could be further explored by an interested student. Perhaps you are a frustrated amateur golfer and want to see how you could sonify your golf swing to tell you how you are doing during the parts of the swing you can't see? You might want to use accelerometers or data from video cameras (which then involves computer vision) to capture the raw data.

  3. Auditory augmentation: This is a technique where the sound made by everyday objects are augmented with extra sonic filtering so that the sound they make changes in response to changes in the values of a data stream you wish to monitor. See [Till Bovermann et al's article](https://www.techfak.uni-bielefeld.de/ags/ami/publications/media/BovermannTuennermannHermann2010-AA.pdf) for more info.

  4. Direct sonification: Much sonification practice relies on metaphor. Recently I have been exploring whether sonifications might work better the more direct (less metaphorical) they are (see [Direct Segmented Sonification](https://paulvickers.github.io/DSSon/), though don't be put off by the signal processing equations as we could do something simpler). An interesting project would be to make two sonifications of the same dataset, one more direct, one more metaphorical, and conduct a user study to see which was better at communicating information to the listener.

  5. Come up with your own idea! I would love to discuss the possibilities with you.


  #### Get in Touch


  If any of these ideas excite you (and they should!) then make an appointment to so that we can work something up. I love this kind of stuff, so I am likely to be very interested in supporting you to develop a really strong project.
---
